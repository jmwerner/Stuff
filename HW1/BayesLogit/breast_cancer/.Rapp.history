var(scale(as.matrix(data[,1:10]))
)
starting_sigma = diag(nrow(data))
starting_sigma
starting_sigma = diag(ncol(data))
starting_sigma
corr(data[,1:10])
cor(data[,1:10])
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 100*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
v_vect = v*diag(length(Beta_0))
v=1
v_vect = v*diag(length(Beta_0))
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	# v_vect = rep(v, length(Beta_0)) #.13 or so#
	# for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .65){#
						# v_vect[par] = v_vect[par]*1.5#
						# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .45){#
							# v_vect[par] = v_vect[par]/1.5#
							# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	# v = 1#
	# for(l in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		# }else{#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(l %% retune)){#
			# current_index = length(acceptance_vector_burnin)#
			# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.4#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .3){#
					# v1=v#
					# v = v/1.4#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #2#
	v_vect = v*diag(length(Beta_0))#
#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 100*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
sigma.inv is input inverse covariance matrix#
"pi_fun" <- function(beta, y, X, m, mu, sigma.inv){#
	return(t(beta - mu) %*% sigma.inv %*% (beta-mu) + #
		t(y) %*% (X %*% beta) - t(m) %*% log(1 + exp(X %*% beta)))#
}#pi_fun
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 100*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
bayes.logreg function#
# beta is vector of starting coefficients (guesses)#
# y is response#
# X is design matrix (size n x p)#
# m is vector of n for each trial#
# Sigma.0.inv is input of inverse covariance matrix#
# niter is the iterations to run the MH algorithm following the burnin#
# burnin is the number of iterations to burn in the algorithm (and retune proposal variance)#
# print.every is the number of iterations between update printings to the user#
# retune is the number of iterations between retunings of proposal variance during burnin period#
# verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	# v_vect = rep(v, length(Beta_0)) #.13 or so#
	# for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .65){#
						# v_vect[par] = v_vect[par]*1.5#
						# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .45){#
							# v_vect[par] = v_vect[par]/1.5#
							# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	# v = 1#
	# for(l in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		# }else{#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(l %% retune)){#
			# current_index = length(acceptance_vector_burnin)#
			# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.4#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .3){#
					# v1=v#
					# v = v/1.4#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #2#
	v_vect = v*diag(length(Beta_0))#
#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	cat("beans")#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 100*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
solve(starting_sigma)
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
Beta_0
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	# v_vect = rep(v, length(Beta_0)) #.13 or so#
	# for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .65){#
						# v_vect[par] = v_vect[par]*1.5#
						# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .45){#
							# v_vect[par] = v_vect[par]/1.5#
							# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	# v = 1#
	# for(l in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		# }else{#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(l %% retune)){#
			# current_index = length(acceptance_vector_burnin)#
			# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.4#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .3){#
					# v1=v#
					# v = v/1.4#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #2#
	v_vect = v*diag(length(Beta_0))#
#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	# v_vect = rep(v, length(Beta_0)) #.13 or so#
	# for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .65){#
						# v_vect[par] = v_vect[par]*1.5#
						# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .45){#
							# v_vect[par] = v_vect[par]/1.5#
							# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	# v = 1#
	# for(l in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		# }else{#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(l %% retune)){#
			# current_index = length(acceptance_vector_burnin)#
			# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.4#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .3){#
					# v1=v#
					# v = v/1.4#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #2#
	v_vect = v*diag(length(Beta_0))#
#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		print(Beta_1)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	# v_vect = rep(v, length(Beta_0)) #.13 or so#
	# for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .65){#
						# v_vect[par] = v_vect[par]*1.5#
						# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .45){#
							# v_vect[par] = v_vect[par]/1.5#
							# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	# v = 1#
	# for(l in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		# }else{#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(l %% retune)){#
			# current_index = length(acceptance_vector_burnin)#
			# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.4#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .3){#
					# v1=v#
					# v = v/1.4#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #2#
	v_vect = v*diag(length(Beta_0))#
	print(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	# v_vect = rep(v, length(Beta_0)) #.13 or so#
	# for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .65){#
						# v_vect[par] = v_vect[par]*1.5#
						# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .45){#
							# v_vect[par] = v_vect[par]/1.5#
							# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	# v = 1#
	# for(l in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		# }else{#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(l %% retune)){#
			# current_index = length(acceptance_vector_burnin)#
			# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.4#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .3){#
					# v1=v#
					# v = v/1.4#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #2#
	v_mat = v*diag(length(Beta_0))#
	print(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	# v_vect = rep(v, length(Beta_0)) #.13 or so#
	# for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .65){#
						# v_vect[par] = v_vect[par]*1.5#
						# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .45){#
							# v_vect[par] = v_vect[par]/1.5#
							# if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	# v = 1#
	# for(l in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		# }else{#
			# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(l %% retune)){#
			# current_index = length(acceptance_vector_burnin)#
			# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.4#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .3){#
					# v1=v#
					# v = v/1.4#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #2#
	v_mat = v*diag(length(Beta_0))#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, tuning_loops = 2, verbose = TRUE)
retune is the number of iterations between retunings of proposal variance during burnin period#
# verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .65){#
						v_vect[par] = v_vect[par]*1.5#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .45){#
							v_vect[par] = v_vect[par]/1.5#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v = 1#
	for(l in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		}else{#
			acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(l %% retune)){#
			current_index = length(acceptance_vector_burnin)#
			percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #2#
	v_mat = v*diag(length(Beta_0))#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, retune = 200, tuning_loops = 5, verbose = TRUE)
retune is the number of iterations between retunings of proposal variance during burnin period#
# verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .65){#
						v_vect[par] = v_vect[par]*1.5#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .45){#
							v_vect[par] = v_vect[par]/1.5#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v = .1#
	for(l in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		}else{#
			acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(l %% retune)){#
			current_index = length(acceptance_vector_burnin)#
			percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #2#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*v_vect#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, retune = 200, tuning_loops = 3, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .65){#
						v_vect[par] = v_vect[par]*1.5#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .45){#
							v_vect[par] = v_vect[par]/1.5#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v = .1#
	for(l in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		}else{#
			acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(l %% retune)){#
			current_index = length(acceptance_vector_burnin)#
			percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #2#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=8000, niter = 15000, retune = 200, tuning_loops = 3, verbose = TRUE)
Breast cancer data analysis based off of other Bayes Logit script#
#
#Packages needed#
library(MASS)#
setwd("/Users/jmwerner1123/Dropbox/GitHub/STA250/Stuff/HW1/BayesLogit/breast_cancer")#
data = read.table("breast_cancer.txt", header = TRUE)#
#
levels(data$diagnosis) = c(0,1) #slick way to reassign 1 and 0 to M and B#
###############################################################
#functions#
###############################################################
##Pi Function, because pi is fun (obviously)#
# beta is vector of coefficients#
# y is response#
# X is design matrix (size n x p)#
# m is vector of n for each trial#
# mu is vector of mu values#
# sigma.inv is input inverse covariance matrix#
"pi_fun" <- function(beta, y, X, m, mu, sigma.inv){#
	return(t(beta - mu) %*% sigma.inv %*% (beta-mu) + #
		t(y) %*% (X %*% beta) - t(m) %*% log(1 + exp(X %*% beta)))#
}#pi_fun#
###############################################################
##bayes.logreg function#
# beta is vector of starting coefficients (guesses)#
# y is response#
# X is design matrix (size n x p)#
# m is vector of n for each trial#
# Sigma.0.inv is input of inverse covariance matrix#
# niter is the iterations to run the MH algorithm following the burnin#
# burnin is the number of iterations to burn in the algorithm (and retune proposal variance)#
# print.every is the number of iterations between update printings to the user#
# retune is the number of iterations between retunings of proposal variance during burnin period#
# verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.3#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.3#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v = .1#
	for(l in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
		}else{#
			acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(l %% retune)){#
			current_index = length(acceptance_vector_burnin)#
			percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #2#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=800, niter = 15000, retune = 200, tuning_loops = 3, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=800, niter = 15000, retune = 100, tuning_loops = 3, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=600, niter = 15000, retune = 200, tuning_loops = 6, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, retune = 200, tuning_loops = 1, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.3#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.3#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 15000, retune = 200, tuning_loops = 1, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 15000, retune = 200, tuning_loops = 2, verbose = TRUE)
apply(X_in[,2:11],2,var)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.3#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.3#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 15000, retune = 200, tuning_loops = 2, verbose = TRUE)
verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.4#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.4#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*2#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/2#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 15000, retune = 200, tuning_loops = 2, verbose = TRUE)
verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 15000, retune = 200, tuning_loops = 2, verbose = TRUE)
"pi_fun" <- function(beta, y, X, m, mu, sigma.inv){#
	return(t(beta - mu) %*% sigma.inv %*% (beta-mu) + #
		t(y) %*% (X %*% beta) - t(m) %*% log(1 + exp(X %*% beta)))#
}#pi_fun#
###############################################################
##bayes.logreg function#
# beta is vector of starting coefficients (guesses)#
# y is response#
# X is design matrix (size n x p)#
# m is vector of n for each trial#
# Sigma.0.inv is input of inverse covariance matrix#
# niter is the iterations to run the MH algorithm following the burnin#
# burnin is the number of iterations to burn in the algorithm (and retune proposal variance)#
# print.every is the number of iterations between update printings to the user#
# retune is the number of iterations between retunings of proposal variance during burnin period#
# verbose is the logical to print updates or no updates#
###############################################################
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 15000, retune = 200, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .4){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #.13 or so#
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.4#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.4#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 10000, retune = 200, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.4#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.4#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 10000, retune = 200, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .65){#
						v_vect[par] = v_vect[par]*1.4#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .35){#
							v_vect[par] = v_vect[par]/1.4#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 10000, retune = 200, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .65){#
						v_vect[par] = v_vect[par]*2#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .35){#
							v_vect[par] = v_vect[par]/2#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 10000, retune = 200, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.8#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.8#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1200, niter = 10000, retune = 200, tuning_loops = 2, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 200, tuning_loops = 10, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 6, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.8#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.8#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 6, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.4#
						if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.4#
							if(verbose){cat(paste("Retuning proposal variance for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	#v_mat = v*diag(length(Beta_0))#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 6, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .6){#
						v_vect[par] = v_vect[par]*1.4#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .4){#
							v_vect[par] = v_vect[par]/1.4#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 6, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.4#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.4#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 6, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.3#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.3#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 6, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 8, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
	#Burn-in Period to get a somewhat close v for all parameters#
	# for(i in 1:burnin){#
		# #proposal density of a normal variable centered at mu#
		# Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		# if(alpha >= log(runif(1))){#
			# Beta_0 = Beta_1#
			# acceptance_vector_burnin[i] = TRUE#
		# }else{#
			# acceptance_vector_burnin[i] = FALSE#
		# }#
		# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		# #Retune#
		# if(!(i %% retune)){#
			# percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			# if(percent_1 > .7){#
				# v1=v#
				# v = v*1.5#
				# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			# }else{#
				# if(percent_1 < .4){#
					# v1=v#
					# v = v/1.5#
					# if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				# }#
			# }#
		# }#
	# }#burnin loop #1#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 8, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v=1#
	Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 8, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v=1#
	Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	v_mat = v*diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg
Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
#
	Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .1#
#
	Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
# # 	for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .8){#
						# v_vect[par] = v_vect[par]*1.2#
						# if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .2){#
							# v_vect[par] = v_vect[par]/1.2#
							# if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
# # 	for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .8){#
						# v_vect[par] = v_vect[par]*1.2#
						# if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .2){#
							# v_vect[par] = v_vect[par]/1.2#
							# if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
# # 	for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .8){#
						# v_vect[par] = v_vect[par]*1.2#
						# if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .2){#
							# v_vect[par] = v_vect[par]/1.2#
							# if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
# # 	for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .8){#
						# v_vect[par] = v_vect[par]*1.2#
						# if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .2){#
							# v_vect[par] = v_vect[par]/1.2#
							# if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(v_vect))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
# # 	for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .8){#
						# v_vect[par] = v_vect[par]*1.2#
						# if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .2){#
							# v_vect[par] = v_vect[par]/1.2#
							# if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg
Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
# # 	for(looper in 1:tuning_loops){#
		# cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		# for(par in 1:length(Beta_0)){#
			# for(k in 1:burnin){#
				# Beta_1 = Beta_0#
				# Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				# alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						# pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				# if(alpha >= log(runif(1))){#
					# Beta_0 = Beta_1#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				# }else{#
					# acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				# }#
				# beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				# #Retune#
				# if(!(k %% retune)){#
					# current_index = length(acceptance_vector_burnin)#
					# percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					# if(percent_1 > .8){#
						# v_vect[par] = v_vect[par]*1.2#
						# if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					# }else{#
						# if(percent_1 < .2){#
							# v_vect[par] = v_vect[par]/1.2#
							# if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						# }#
					# }#
				# }#
			# }	#
		# }#
	# } #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=400, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 10000, retune = 100, tuning_loops = 2, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
 	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .8){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .2){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 10000, retune = 100, tuning_loops = 3, verbose = TRUE)
summary(betas)
means = apply(betas, 2, mean)
means
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 10000, retune = 100, tuning_loops = 7, verbose = TRUE)#
means = apply(betas, 2, mean)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
 	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .7){#
						v_vect[par] = v_vect[par]*1.2#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .3){#
							v_vect[par] = v_vect[par]/1.2#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 10000, retune = 100, tuning_loops = 5, verbose = TRUE)
"bayes.logreg" <- function(m,y,X,beta.0,Sigma.0.inv, niter=10000,burnin=1000, print.every=1000,retune=100, tuning_loops = 1, verbose=TRUE){#
	if(verbose){cat(paste("Begin bayes.logreg function with", burnin,#
		"burn in iterations \n and", niter, "iterations following the burn in period\nCurrent time:", date(), "\n\n"))}#
	begin_time = Sys.time()#
	Beta_0 = beta.0#
	#Proposal variance starting point #
	v = .4#
#
	#Burn-in Period to get a somewhat close v for all parameters#
	for(i in 1:burnin){#
		#proposal density of a normal variable centered at mu#
		Beta_1 = mvrnorm(1,Beta_0, v*diag(length(Beta_0)))#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector_burnin[i] = TRUE#
		}else{#
			acceptance_vector_burnin[i] = FALSE#
		}#
		beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
		#Retune#
		if(!(i %% retune)){#
			percent_1 = sum(acceptance_vector_burnin[(i-retune+1):i])/retune#
			if(percent_1 > .7){#
				v1=v#
				v = v*1.5#
				if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
			}else{#
				if(percent_1 < .3){#
					v1=v#
					v = v/1.5#
					if(verbose){cat(paste("Retuning proposal variance to", round(v,4), "from", round(v1,4), "\n"))}#
				}#
			}#
		}#
	}#burnin loop #1#
	acceptance_vector_burnin = c()#
	beta_vector_burnin = c()#
#################################################################################	#
#burn-in period to tune each parameter's proposal variance closer to where it needs to be#
	v_vect = rep(v, length(Beta_0)) #
 	for(looper in 1:tuning_loops){#
		cat(paste("Starting tuning loop", looper, "of", tuning_loops, "\n"))#
		for(par in 1:length(Beta_0)){#
			for(k in 1:burnin){#
				Beta_1 = Beta_0#
				Beta_1[par] = rnorm(1,Beta_0[par], sqrt(v_vect[par]))#
				alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
						pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
				if(alpha >= log(runif(1))){#
					Beta_0 = Beta_1#
					acceptance_vector_burnin = c(acceptance_vector_burnin, TRUE)#
				}else{#
					acceptance_vector_burnin = c(acceptance_vector_burnin, FALSE)#
				}#
				beta_vector_burnin = rbind(beta_vector_burnin, Beta_0)		#
				#Retune#
				if(!(k %% retune)){#
					current_index = length(acceptance_vector_burnin)#
					percent_1 = sum(acceptance_vector_burnin[(current_index-retune+1):current_index])/retune#
					if(percent_1 > .85){#
						v_vect[par] = v_vect[par]*1.15#
						if(verbose){cat(paste("Retuning proposal variance up for beta", par, "\n"))}#
					}else{#
						if(percent_1 < .15){#
							v_vect[par] = v_vect[par]/1.15#
							if(verbose){cat(paste("Retuning proposal variance down for beta", par, "\n"))}#
						}#
					}#
				}#
			}	#
		}#
	} #tuning looper#
	v_mat = diag(v_vect)#
	#Post burn-in Period#
	if(verbose){cat("\nBurn in period complete. \n \n")}#
	acceptance_vector = c()#
	beta_vector = c()#
	for(t in 1:niter){#
		if(!(t %% print.every)){cat(paste("Algorithm is", t/niter*100, #
			"percent complete and has been running for", round(as.numeric(Sys.time()-begin_time),2),"seconds\n"))}#
		Beta_1 = mvrnorm(1,Beta_0, v_mat)#
		alpha = min(0,pi_fun(Beta_1, y, X, m, beta.0, Sigma.0.inv) -#
					pi_fun(Beta_0, y, X, m, beta.0, Sigma.0.inv))#
		if(alpha >= log(runif(1))){#
			Beta_0 = Beta_1#
			acceptance_vector[t] = TRUE#
		}else{#
			acceptance_vector[t] = FALSE#
		}	#
		beta_vector = rbind(beta_vector, Beta_0)					#
	}#
	if(verbose){#
		plot.new()#
		par(mfrow = c(3,4))#
		for(i in 1:(ncol(data))){#
			plot(beta_vector[,i], type = "l", main = paste("Beta", i))#
		}#
		hist(as.numeric(acceptance_vector))#
	}#
	end_time = Sys.time()#
	if(verbose){cat(paste("\nAcceptance rate:", sum(acceptance_vector)/length(acceptance_vector), "\nElapsed time:", round(as.numeric(end_time - begin_time),3), "seconds"))} #ending verbose output#
	return(beta_vector)#
}#bayes.logreg#
###############################################################
#
#############
#Main block##
#############
#Standardize X columns for numerical stability#
X_in = cbind(rep(1,nrow(data)),scale(as.matrix(data[,1:10])))#
#X_in = cbind(rep(1,nrow(data)),as.matrix(data[,1:10]))#
#
starting_sigma = 1000*diag(ncol(data))#
starting_b0 = apply(X_in, 2, "mean")#
#niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=1000, niter = 10000, retune = 100, tuning_loops = 7, verbose = TRUE)
acf(betas[,1], plot = FALSE)
acf(betas[,1], plot = FALSE)[1]
niter, print.every, and retune defaults are acceptable and will not be changed here#
betas = bayes.logreg(rep(1,nrow(data)), as.numeric(data$diagnosis)-1, X_in, starting_b0, solve(starting_sigma),burnin=800, niter = 15000, retune = 100, tuning_loops = 7, verbose = TRUE)
summary(betas)
means = apply(betas, 2, mean)
autocorrs = apply(betas, 2, function(x) acf(x, plot=FALSE)[1])
autocorrs
dim(autocorrs)
length(autocorrs)
autocorrs[1]
autocorrs[4]
autocorrs[9]
as.numeric(autocorrs)
as.numeric(unlist(autocorrs))
data
u = exp(X%*%means)/(1+exp(X%*%means))
X = cbind(rep(1,nrow(data)),data[1:10])
u = exp(X%*%means)/(1+exp(X%*%means))
means
class(means)
dim(means)
X
class(X)
X = as.matrix(cbind(rep(1,nrow(data)),data[1:10]))
u = exp(X%*%means)/(1+exp(X%*%means))
u
u = exp(X_in%*%means)/(1+exp(X_in%*%means))
u
?rbinom
u[1]
simulated_data = rbinom(5000,1,u[1])
plot(density(simulated_data))
head(simulated_data)
Posterior predictive checks using the mean of our returned beta matrix#
u = exp(X_in%*%means)/(1+exp(X_in%*%means))#
simulated_data = list()#
for(i in 1:length(u)){#
	simulated_data[[i]] = rbinom(500,1,u[i])#
}
stats = lapply(simulated_data, "mean")
stats
means = apply(betas, 2, mean)#
#
autocorrs = apply(betas, 2, function(x) acf(x, plot=FALSE)[1])#
#
#Posterior predictive checks using the mean of our returned beta matrix#
u = exp(X_in%*%means)/(1+exp(X_in%*%means))#
simulated_data = list()#
for(i in 1:length(u)){#
	simulated_data[[i]] = rbinom(5000,1,u[i])#
}#
#
stats = lapply(simulated_data, "mean")#
hist(stats)
class(stats)
hist(unlist(stats))
hist(u)
autocorrs = apply(betas, 2, function(x) acf(x, plot=FALSE)[1])
autocorrs
class(autocorrs)
unlist(autocorrs)
a = unlist(autocorrs)
a
names(a)
a$acf
a[acf]
autocorrs = apply(betas, 2, function(x)acf(x, plot=FALSE))
autocorrs
autocorrs[1]
autocorrs[[1]]
autocorrs[[1]][1]
as.numeric(autocorrs[[1]][1])
unlist(autocorrs[[1]][1])
unlist(autocorrs[[1]][1])$acf
unlist(autocorrs[[1]][1])
a=unlist(autocorrs[[1]][1])
a
class(a)
names(a)
a$acf
as.numeric(autocorrs[[1]])
unlist(autocorrs[[1]])
autocorrs = apply(betas, 2, function(x)acf(x, plot=FALSE))
autocorrs = apply(betas, 2, function(x)acf(x, plot=FALSE)[1])
unlist(autocorrs[[1]])
as.numeric(unlist(autocorrs[[1]]))
as.numeric(unlist(autocorrs[[1]]))[1]
acf(x, plot=FALSE)[1]
acf(betas[,1], plot=FALSE)[1]
as.numeric(acf(betas[,1], plot=FALSE)[1])
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[1])))
autocorrs
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[1])))[1,]
autocorrs
library(xtable)
print(xtable(autocorrs), row.names = FALSE)
require(xtable)
print(xtable(autocorrs), row.names = FALSE)
autocorrs
class(autocorrs)
print(xtable(data.frame(autocorrs)), row.names = FALSE)
means = apply(betas, 2, mean)#
#
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[2])))[1,]#
print(xtable(data.frame(autocorrs)), row.names = FALSE)
autocorrs
print(xtable(data.frame(autocorrs),digits=4), row.names = FALSE)
dim(betas)
Posterior predictive checks using the mean of our returned beta matrix#
u = exp(X_in%*%means)/(1+exp(X_in%*%means))#
simulated_data = list()#
for(i in 1:length(u)){#
	simulated_data[[i]] = rbinom(5000,1,u[i])#
}#
#
stats = lapply(simulated_data, "mean")#
hist(unlist(stats))
dim(betas)
j=1
u = exp(X_in[j,]%*%betas[j,])/(1+exp(X_in[j,]%*%betas[j,]))
u
head(data)
summary(data$diagnosis)
Posterior predictive checks using the mean of our returned beta matrix#
for(j in 1:nrows(betas)){#
	u = exp(X_in[j,]%*%betas[j,])/(1+exp(X_in[j,]%*%betas[j,]))#
	simulated_data = list()#
	for(i in 1:length(u)){#
		simulated_data[[i]] = rbinom(nrow(data),1,u[i])#
	}	#
}
means = apply(betas, 2, mean)#
#
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[2])))[1,]#
print(xtable(data.frame(autocorrs),digits=4), row.names = FALSE)#
#
#Posterior predictive checks using the mean of our returned beta matrix#
for(j in 1:nrow(betas)){#
	u = exp(X_in[j,]%*%betas[j,])/(1+exp(X_in[j,]%*%betas[j,]))#
	simulated_data = list()#
	for(i in 1:length(u)){#
		simulated_data[[i]] = rbinom(nrow(data),1,u[i])#
	}	#
}
means = apply(betas, 2, mean)#
#
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[2])))[1,]#
print(xtable(data.frame(autocorrs),digits=4), row.names = FALSE)#
#
#Posterior predictive checks using the mean of our returned beta matrix#
for(j in 1:nrow(betas)){#
	u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))#
	simulated_data = list()#
	for(i in 1:length(u)){#
		simulated_data[[i]] = rbinom(nrow(data),1,u[i])#
	}	#
}
u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))
u
nrow(betas)
means = apply(betas, 2, mean)#
#
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[2])))[1,]#
print(xtable(data.frame(autocorrs),digits=4), row.names = FALSE)#
#
#Posterior predictive checks using the mean of our returned beta matrix#
simulated_data = c()#
for(j in 1:nrow(betas)){#
	u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))#
	for(i in 1:length(u)){#
		simulated_data = cbind(simulated_data,rbinom(nrow(data),1,u[i]))#
	}	#
	if(!(j%%1000)){cat(paste(j/nrow(betas), "Percent done\n"))}#
}
j
u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))
u
summary(u)
simulated_data
rbinom(length(u),1,u[i])
rbinom(length(u),rep(1, length(u)),u)
rbinom(3,1, c(0,1))
rbinom(3,1, c(0,1,1))
rbinom(3,1, c(0,1,1))
rbinom(3,1, c(0,1,1))
rbinom(3,1, c(0,1,1))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
rbinom(3,1, c(0,1,.5))
a = c(1,2,3,4,5)
b=c(5,4,3,2,1)
a>b
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[2])))[1,]#
print(xtable(data.frame(autocorrs),digits=4), row.names = FALSE)#
#
#Posterior predictive checks using the mean of our returned beta matrix#
simulated_data = c()#
for(j in 1:nrow(betas)){#
	u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))#
	for(i in 1:length(u)){#
		simulated_data = cbind(simulated_data,rbinom(length(u),1,u))#
	}	#
	if(!(j%%1000)){cat(paste(j/nrow(betas), "Percent done\n"))}#
}
Posterior predictive checks using the mean of our returned beta matrix#
simulated_data = c()#
for(j in 1:nrow(betas)){#
	u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))#
	for(i in 1:length(u)){#
		simulated_data = cbind(simulated_data,rbinom(length(u),1,u))#
	}	#
	cat(j)#
	if(!(j%%1000)){cat(paste(j/nrow(betas), "Percent done\n"))}#
}
u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))
u
rbinom(length(u),1,u)
autocorrs = apply(betas, 2, function(x)as.numeric(unlist(acf(x, plot=FALSE)[2])))[1,]#
print(xtable(data.frame(autocorrs),digits=4), row.names = FALSE)#
#
#Posterior predictive checks using the mean of our returned beta matrix#
simulated_data = c()#
for(j in 1:nrow(betas)){#
	u = exp(X_in%*%betas[j,])/(1+exp(X_in%*%betas[j,]))		#
	simulated_data = cbind(simulated_data,rbinom(length(u),1,u))#
	if(!(j%%1000)){cat(paste(j/nrow(betas), "Percent done\n"))}#
}
stats = apply(simulated_data, "mean")
stats = apply(simulated_data,2, "mean")
hist(stats)
abline(v=mean(data$response), col = "red", lwd = 2)
names(data)
abline(v=mean(data$diagnosis), col = "red", lwd = 2)
mean(data$diagnosis)
data$diagnosis
as.numeric(data$diagnosis)
abline(v=mean(as.numeric(data$diagnosis)-1), col = "red", lwd = 2)
stats = apply(simulated_data,2, "mean")#
pdf("ppc.pdf")#
hist(stats, main = "Posterior Predictive Check for Mean")#
abline(v=mean(as.numeric(data$diagnosis)-1), col = "red", lwd = 2, lty = 2)#
dev.off()
stats = apply(simulated_data,2, "median")#
hist(stats, main = "Posterior Predictive Check for Mean")#
abline(v=median(as.numeric(data$diagnosis)-1), col = "red", lwd = 2, lty = 2)
stats
stats = apply(simulated_data,2, "sd")
stats
stats = apply(simulated_data,2, "sd")#
hist(stats, main = "Posterior Predictive Check for Mean")#
abline(v=sd(as.numeric(data$diagnosis)-1), col = "red", lwd = 2, lty = 2)
stats = apply(simulated_data,2, "mean")#
pdf("ppc.pdf")#
hist(stats, main = "Posterior Predictive Check for Mean")#
abline(v=mean(as.numeric(data$diagnosis)-1), col = "red", lwd = 2, lty = 2)#
legend("topleft", c("Real Data"), lty = c(2), lwd = c(2), col = c("red"))#
dev.off()#
stats = apply(simulated_data,2, "sd")#
pdf("ppc_sd.pdf")#
hist(stats, main = "Posterior Predictive Check for Standard Deviation")#
abline(v=sd(as.numeric(data$diagnosis)-1), col = "red", lwd = 2, lty = 2)#
legend("topleft", c("Real Data"), lty = c(2), lwd = c(2), col = c("red"))#
dev.off()
getwd()
